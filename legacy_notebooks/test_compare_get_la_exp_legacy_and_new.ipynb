{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  instead of importing loopify, let's %run it:\n",
    "# %run /data/venevs/loops_stuff/cooloop/loopify.py\n",
    "# %run /data/venevs/loops_stuff/cooloop/old_loopify.py\n",
    "# %run /data/venevs/loops_stuff/cooloop/plotools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions ...\n",
    "import cooler\n",
    "import cooltools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from scipy.stats import poisson\n",
    "# from cooltools.plotools import show_heatmap_wfeat, get_kernel\n",
    "\n",
    "from cooltools.loopify import diagonal_matrix_tiling, \\\n",
    "                              tile_of_expected, \\\n",
    "                              get_adjusted_expected_tile_some_nans, \\\n",
    "                              multiple_test_BH, \\\n",
    "                              clust_2D_pixels, \\\n",
    "                              square_matrix_tiling\n",
    "                \n",
    "                \n",
    "from cooltools.old_loopify import get_adjusted_expected, \\\n",
    "                                  get_adjusted_expected_some_nans, \\\n",
    "                                  get_adjusted_expected_tile\n",
    "                \n",
    "from cooltools.snipping import LazyToeplitz\n",
    "            \n",
    "from cooltools.plotools import get_kernel\n",
    "            \n",
    "from copy import deepcopy\n",
    "from bioframe import parse_humanized, parse_region_string\n",
    "# from scipy.ndimage import convolve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs for `call_loops_per_chrom`\n",
    "\n",
    "Prepare a nice cooler-file, chromosome of interest, expected in `pd.DataFrame` format, and for now, just a single `kernels=[kernel, ]` to perform peak-calling against locally-adjusted expected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin-type fixed\n",
      "bin-size 20000\n",
      "nchroms 25\n",
      "nbins 154795\n",
      "sum 1106009388\n",
      "nnz 510537257\n",
      "genome-assembly hg19\n",
      "metadata {}\n",
      "creation-date 2017-12-11T06:24:11.564370\n",
      "generated-by cooler-0.7.5\n",
      "format HDF5::Cooler\n",
      "format-version 2\n",
      "format-url https://github.com/mirnylab/cooler\n"
     ]
    }
   ],
   "source": [
    "# cname = './houda/Houda_Ctrl_DpnII_K562.20000.cool'\n",
    "# cname = './houda/Houda_Ctrl_DpnII_K562.10000.cool'\n",
    "cname = './anne-laure/PTB2539-NT.20000.cool'\n",
    "ename = cname.rstrip(\".cool\") + \".cis.expected\"\n",
    "the_c = cooler.Cooler(cname)\n",
    "for k,v in the_c.info.items():\n",
    "    print(k,v)\n",
    "# resolution aka bin-size,\n",
    "# typically 5000,10000,20000 (5-20 KB)\n",
    "b = the_c.info['bin-size']\n",
    "\n",
    "#chromosome of interest\n",
    "chrom = \"chr17\"\n",
    "\n",
    "# # Run CLI expected when needed:\n",
    "# !cooltools compute_expected --contact-type cis --nproc 18 $cname > $ename\n",
    "cis_exp = pd.read_table(ename,index_col=[0,1])\n",
    "# # trans expected, just for the record:\n",
    "# !cooltools compute_expected --contact-type trans PTB2539-NT.200000.cool > PTB2539-NT.200000.trans.expected\n",
    "# trans_exp = pd.read_table(\"PTB2539-NT.200000.trans.expected\",index_col=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donut mask\n",
    "\n",
    "let's create a donut shape mask for downstream analysis ...\n",
    "\n",
    "We're exploring 10KB Hi-C matrix in this example, but donut-mask parameters, probably must be different for 10kb, look up Rao et el. 2014.\n",
    "\n",
    "params are not the most efficient, perhaps ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAD/CAYAAABCS0s6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e5dec6710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = 3\n",
    "p = 1\n",
    "kernel = get_kernel(w,p,ktype='donut')\n",
    "\n",
    "# kernel/kernel.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the loopify:\n",
    "\n",
    "Let's finally try to call peaks using loopify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.linalg import toeplitz\n",
    "\n",
    "# assuming cooler, chrom and expected as input ...\n",
    "ice_v_name = \"weight\"\n",
    "exp_v_name = \"balanced.avg\"\n",
    "\n",
    "M_raw = the_c.matrix(balance=False).fetch(chrom)\n",
    "M_ice = the_c.matrix().fetch(chrom)\n",
    "M_raw = the_c.matrix(balance=False).fetch(chrom)\n",
    "v_ice = the_c.bins().fetch(chrom)[ice_v_name].as_matrix()\n",
    "\n",
    "exp_v = cis_exp.loc[chrom][exp_v_name].values\n",
    "# We'll reconstruct a 2D expected matrix from \n",
    "# the `exp_v` for ease of coding for now, \n",
    "# but overall it is another major inefficiency\n",
    "# of the code (memory-footprint etc).\n",
    "E_ice = toeplitz(exp_v)\n",
    "# GLOBAL expected:\n",
    "# deiced E_ice: element-wise division of E_ice[i,j] and\n",
    "# v_ice[i]*v_ice[j]:\n",
    "E_raw = np.divide(E_ice, np.outer(v_ice,v_ice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunking attempt \n",
    "\n",
    "try doing the same as above (extracting dense matrices), only for small chunks along the diagonal this time around\n",
    "```\n",
    "\n",
    "    * * * * * * * * * * *  0-th slice\n",
    "    *       *           *\n",
    "    *       *           *\n",
    "    *   * * * * *       *  i-th slice\n",
    "    *   *   *   *       *\n",
    "    * * * * *   *       *\n",
    "    *   *       *       *\n",
    "    *   * * * * *       *\n",
    "    *                   *\n",
    "    *              ...  *  ...\n",
    "    *                   *\n",
    "    *                   *\n",
    "    * * * * * * * * * * *\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# little side-step to generate the test-case\n",
    "# that we use in the cooloop test-suite ... \n",
    "###################################\n",
    "L,L = M_ice.shape\n",
    "M = int(parse_humanized('2M')/b)\n",
    "# w-edge ...\n",
    "\n",
    "T = L//M + bool(L%M)\n",
    "\n",
    "tiles_origin = []\n",
    "tiles_M_ice = []\n",
    "tiles_M_raw = []\n",
    "tiles_E_ice = []\n",
    "tiles_v_ice = []\n",
    "# by doing range(1,T) we are making\n",
    "# sure we are processing the upper-left\n",
    "# chunk only once:\n",
    "for t in range(1,T):\n",
    "    # l = max(0,M*t-M)\n",
    "    # r = min(L,M*t+M)\n",
    "    lw = max(0,M*t-M-w)\n",
    "    rw = min(L,M*t+M+w)\n",
    "    #\n",
    "    origin_lw = (lw,lw)\n",
    "    tiles_origin.append(origin_lw)\n",
    "    tiles_M_ice.append(M_ice[lw:rw,lw:rw])\n",
    "    tiles_M_raw.append(M_raw[lw:rw,lw:rw])\n",
    "    tiles_E_ice.append(E_ice[lw:rw,lw:rw])\n",
    "    tiles_v_ice.append(v_ice[lw:rw])\n",
    "\n",
    "# # there you go!\n",
    "# # here are your tiles ...\n",
    "# print(np.isfinite(tiles_M_ice[15]).sum())\n",
    "\n",
    "mock_M_raw = tiles_M_raw[28]\n",
    "mock_M_ice = tiles_M_ice[28]\n",
    "mock_E_ice = tiles_E_ice[28]\n",
    "mock_v_ice = tiles_v_ice[28]\n",
    "# mock_M_ice = tiles_M_ice[15]\n",
    "\n",
    "##############################\n",
    "# new stuff for the cooloop test only:\n",
    "##############################\n",
    "\n",
    "# # kernels=(kernel,)\n",
    "# b=the_c.info['bin-size']\n",
    "\n",
    "# ####################################################\n",
    "# # # first, generate that locally-adjusted expected:\n",
    "# # # MOCK-results saving:\n",
    "# ####################################################\n",
    "# # # Ed_raw, mask_ndx, Cobs, Cexp, NN = \n",
    "# # res = get_adjusted_expected(observed=mock_M_ice,\n",
    "# #                      expected=mock_E_ice,\n",
    "# #                      ice_weight=mock_v_ice,\n",
    "# #                      kernel=kernel,\n",
    "# #                      b=b,\n",
    "# #                      return_type=\"sparse\")\n",
    "# # \n",
    "# # res[['row','col','expected','la_expected','observed']].to_csv(\"mock_res.csv.gz\",compression=\"gzip\",index=False)\n",
    "\n",
    "# np.savez_compressed('mock_inputs',\n",
    "#                     mock_M_raw=mock_M_raw,\n",
    "#                     mock_M_ice=mock_M_ice,\n",
    "#                     mock_E_ice=mock_E_ice,\n",
    "#                     mock_v_ice=mock_v_ice)\n",
    "\n",
    "# x_load = np.load('mock_inputs.npz')\n",
    "# print(np.isclose(x_load['mock_M_raw'] ,mock_M_raw,equal_nan=True).all())\n",
    "# print(np.isclose(x_load['mock_M_ice'] ,mock_M_ice,equal_nan=True).all())\n",
    "# print(np.isclose(x_load['mock_E_ice'] ,mock_E_ice,equal_nan=True).all())\n",
    "# print(np.isclose(x_load['mock_v_ice'] ,mock_v_ice,equal_nan=True).all())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to be continued from here on ...\n",
    "\n",
    "it is all updated up until this point,\n",
    "but below it is still old, carrying a lot of \n",
    "things from retired loopify etc.\n",
    "\n",
    "get inspiration from cooloop tests to update \n",
    "the sections below ...\n",
    "\n",
    "\n",
    "## todo:\n",
    "\n",
    " - separate legacy code\n",
    " - cleanup the notebook\n",
    " - combine into 1 pipeline ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 'get_adjusted_expected_tile' for a bunch of tiles ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "kernels convolved with observed and expected ...\n",
      "    Run 'get_adjusted_expected_some_nans' entire chromosome ...\n",
      "kernels convolved with observed and expected ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venevs/miniconda3/lib/python3.6/site-packages/cooltools/old_loopify.py:521: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Ed_raw = np.multiply(E_raw, np.divide(KM, KE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run 'get_adjusted_expected' entire chromosome (the most original la_exp) ...\n",
      "kernels convolved with observed and expected ...\n",
      "mask_Ed include all elements of mask_M (expected)\n",
      "mask_Ed include all elements of mask_NN (expected)\n",
      "But mask_Ed!=mask_NN (expected e.g. for NN>=2)\n",
      "If all test yield as expected, masking is practically useless ...\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# testing 3 legacy la_exp functions ...\n",
    "#############################\n",
    "\n",
    "nans_to_tolerate = 1\n",
    "band_2Mb = 2e+6\n",
    "kernels=(kernel,)\n",
    "b=the_c.info['bin-size']\n",
    "\n",
    "print(\"    Run 'get_adjusted_expected_tile' for a bunch of tiles ...\")\n",
    "\n",
    "peak_tiles = []\n",
    "for origin,observed,expected,ice_weight in zip(tiles_origin,\n",
    "                                               tiles_M_ice,\n",
    "                                               tiles_E_ice,\n",
    "                                               tiles_v_ice):\n",
    "    peak_tile = get_adjusted_expected_tile(origin=origin,\n",
    "                                               observed=observed,\n",
    "                                               expected=expected,\n",
    "                                               ice_weight=ice_weight,\n",
    "                                               kernels=kernels,\n",
    "                                               b=b,\n",
    "                                               band=band_2Mb)\n",
    "    peak_tiles.append(peak_tile)\n",
    "\n",
    "peaks_tiles_concat = pd.concat(peak_tiles,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"    Run 'get_adjusted_expected_some_nans' entire chromosome ...\")\n",
    "\n",
    "# first, generate that locally-adjusted expected:\n",
    "# Ed_raw, mask_ndx, Cobs, Cexp, NN\n",
    "peaks_nans = get_adjusted_expected_some_nans(\n",
    "                                        observed = M_ice,\n",
    "                                        expected = E_ice,\n",
    "                                        ice_weight = v_ice,\n",
    "                                        kernels = kernels,\n",
    "                                        b = b,\n",
    "                                        band = band_2Mb,\n",
    "                                        nan_threshold=nans_to_tolerate\n",
    "                                       )\n",
    "\n",
    "\n",
    "print(\"    Run 'get_adjusted_expected' entire chromosome (the most original la_exp) ...\")\n",
    "\n",
    "\n",
    "peaks_original = get_adjusted_expected(\n",
    "                             observed=M_ice,\n",
    "                             expected=E_ice,\n",
    "                             ice_weight=v_ice,\n",
    "                             kernel=kernel,\n",
    "                             b=b,\n",
    "                             return_type=\"sparse\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing new tiling strategies:\n",
    "\n",
    " - diag\n",
    " - square tiling \n",
    " \n",
    " very awesome\n",
    " \n",
    " very cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)   Running the newest 'get_la_exp_tiles_nans' on an entire chrom (single piece) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venevs/miniconda3/lib/python3.6/site-packages/cooltools/loopify.py:659: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Ek_raw = np.multiply(E_raw, np.divide(KO, KE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "(2)   Running the newest 'get_la_exp_tiles_nans' with diagonal chunking ...\n",
      "matrix of size 4060X4060 to be splitted so that\n",
      "  diagonal region of size 100 would be completely\n",
      "  covered by the tiling, additionally keeping\n",
      "  a small 'edge' of size w=3, to allow for\n",
      "  meaningfull convolution around boundaries.\n",
      "  Resulting number of tiles is 40\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "(3)   Running the newest 'get_la_exp_tiles_nans' with arbitrary square chunking ...\n",
      "matrix of size 4060X4060 to be splitted\n",
      "  into square tiles of size 600.\n",
      "  A small 'edge' of size w=3 is added, to allow for\n",
      "  meaningfull convolution around boundaries.\n",
      "  Resulting number of tiles is 49\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n",
      "Convolution with kernel donut is complete.\n",
      "Convolution with kernel footprint is complete.\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# exp_getter = lambda start,stop,shift: cis_exp.iloc[start+shift:stop+shift][exp_v_name].values\n",
    "#################################################\n",
    "chr_start,chr_stop = the_c.extent(chrom)\n",
    "b = the_c.info['bin-size']\n",
    "band_2Mb = 2e+6\n",
    "band_idx = int(band_2Mb/b)\n",
    "w = 3\n",
    "nans_tolerated = 1\n",
    "tile_size = int(12e6/b)\n",
    "verbosity = True\n",
    "lazy_exp = LazyToeplitz(cis_exp.loc[chrom][exp_v_name].values)\n",
    "\n",
    "###############\n",
    "# now with 2 kernels after update\n",
    "# it takes forever on an entire matrix ...\n",
    "###############\n",
    "\n",
    "#################################################\n",
    "# test the newest 'get_adjusted_expected_tile_some_nans'\n",
    "# for the entire chrom , just in case ...\n",
    "#################################################\n",
    "print(\"(1)   Running the newest 'get_la_exp_tiles_nans' on an entire chrom (single piece) ...\")\n",
    "\n",
    "# let's keep X,Y-part explicit here:\n",
    "origin = (chr_start, chr_start)\n",
    "# RAW observed matrix slice:\n",
    "observed = the_c.matrix(balance=False)[chr_start:chr_stop,chr_start:chr_stop]\n",
    "# trying new expected function:\n",
    "expected = lazy_exp[chr_start:chr_stop,chr_start:chr_stop]\n",
    "#################################################\n",
    "# expected = tile_of_expected(chr_start,\n",
    "#                             (chr_start, chr_stop),\n",
    "#                             (chr_start, chr_stop),\n",
    "#                             exp_getter)\n",
    "#################################################\n",
    "# that's the main working function from loopify:\n",
    "res_df = get_adjusted_expected_tile_some_nans(origin = origin,\n",
    "                                         observed = observed,\n",
    "                                         expected = expected,\n",
    "                                         bal_weight = the_c.bins()[chr_start:chr_stop][ice_v_name].values,\n",
    "                                         kernels = {\"donut\":kernel, \"footprint\":np.ones_like(kernel)},\n",
    "                                         verbose = verbosity)\n",
    "is_inside_band = (res_df[\"row\"]>(res_df[\"col\"]-band_idx))\n",
    "does_comply_nans = (res_df[\"la_exp.\"+\"footprint\"+\".nnans\"] < nans_tolerated)\n",
    "# so, selecting inside band and nNaNs compliant results:\n",
    "res_df = res_df[is_inside_band & does_comply_nans].reset_index(drop=True)\n",
    "# adjust the origin\n",
    "res_df['row'] = res_df['row'] - chr_start\n",
    "res_df['col'] = res_df['col'] - chr_start\n",
    "# save using a distinct name:\n",
    "new_peaks_entire_chrom = res_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "# test the newest 'get_adjusted_expected_tile_some_nans'\n",
    "# using diagonal chunking here ...\n",
    "#################################################\n",
    "print(\"(2)   Running the newest 'get_la_exp_tiles_nans' with diagonal chunking ...\")\n",
    "\n",
    "res_df = pd.DataFrame([])\n",
    "for tile in diagonal_matrix_tiling(chr_start, chr_stop, w, band = band_idx):\n",
    "    tilei = tile\n",
    "    tilej = tile\n",
    "    # let's keep i,j-part explicit here:\n",
    "    origin = (tilei[0], tilej[0])\n",
    "    # RAW observed matrix slice:\n",
    "    observed = the_c.matrix(balance=False)[slice(*tilei), slice(*tilej)]\n",
    "    # expected as a tile :\n",
    "    expected = lazy_exp[slice(*tilei), slice(*tilej)]\n",
    "    #################################################\n",
    "    # expected = tile_of_expected(chr_start, tilei, tilej, exp_getter)\n",
    "    #################################################\n",
    "    ice_weight = the_c.bins()[slice(*tile)][ice_v_name].values\n",
    "    # that's the main working function from loopify:\n",
    "    res = get_adjusted_expected_tile_some_nans(origin = origin,\n",
    "                                             observed = observed,\n",
    "                                             expected = expected,\n",
    "                                             bal_weight = ice_weight,\n",
    "                                             kernels = {\"donut\":kernel, \"footprint\":np.ones_like(kernel)},\n",
    "                                             verbose = verbosity)\n",
    "    is_inside_band = (res[\"row\"]>(res[\"col\"]-band_idx))\n",
    "    does_comply_nans = (res[\"la_exp.\"+\"footprint\"+\".nnans\"] < nans_tolerated)\n",
    "    # so, selecting inside band and nNaNs compliant results:\n",
    "    res_df = res_df.append(\n",
    "                res[is_inside_band & does_comply_nans],\n",
    "                ignore_index=True)\n",
    "# drop dups and reset index:\n",
    "res_df = res_df.drop_duplicates().reset_index(drop=True)\n",
    "# adjust the origin :\n",
    "res_df['row'] = res_df['row'] - chr_start\n",
    "res_df['col'] = res_df['col'] - chr_start\n",
    "# save using a distinct name:\n",
    "new_peaks_diag_tiles = res_df\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "# test the newest 'get_adjusted_expected_tile_some_nans'\n",
    "# using square chunking here (the most arbitrary) ...\n",
    "#################################################\n",
    "print(\"(3)   Running the newest 'get_la_exp_tiles_nans' with arbitrary square chunking ...\")\n",
    "\n",
    "res_df = pd.DataFrame([])\n",
    "for tilei, tilej in square_matrix_tiling(chr_start, chr_stop, tile_size = tile_size, edge = w, square = False):\n",
    "    # let's keep i,j-part explicit here:\n",
    "    origin = (tilei[0], tilej[0])\n",
    "    # RAW observed matrix slice:\n",
    "    observed = the_c.matrix(balance=False)[slice(*tilei),slice(*tilej)]\n",
    "    # trying new expected function:\n",
    "    expected = lazy_exp[slice(*tilei), slice(*tilej)]\n",
    "    ###################################################\n",
    "    # expected = tile_of_expected(chr_start, tilei, tilej, exp_getter)\n",
    "    ###################################################\n",
    "    ice_weight_i = the_c.bins()[slice(*tilei)][ice_v_name].values\n",
    "    ice_weight_j = the_c.bins()[slice(*tilej)][ice_v_name].values\n",
    "    # that's the main working function from loopify:\n",
    "    res = get_adjusted_expected_tile_some_nans(origin = origin,\n",
    "                                             observed = observed,\n",
    "                                             expected = expected,\n",
    "                                             bal_weight = (ice_weight_i, ice_weight_j),\n",
    "                                             kernels = {\"donut\":kernel, \"footprint\":np.ones_like(kernel)},\n",
    "                                             verbose=verbosity)\n",
    "    is_inside_band = (res[\"row\"]>(res[\"col\"]-band_idx))\n",
    "    does_comply_nans = (res[\"la_exp.\"+\"footprint\"+\".nnans\"] < nans_tolerated)\n",
    "    # so, selecting inside band and nNaNs compliant results + appending:\n",
    "    res_df = res_df.append(\n",
    "                res[is_inside_band & does_comply_nans],\n",
    "                ignore_index=True)\n",
    "# drop dups and reset index:\n",
    "res_df = res_df.drop_duplicates().reset_index(drop=True)\n",
    "# adjust the origin:\n",
    "res_df['row'] = res_df['row'] - chr_start\n",
    "res_df['col'] = res_df['col'] - chr_start\n",
    "# save using a distinct name:\n",
    "new_peaks_square_tiles = res_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation between different get_la_exp methods ...\n",
    "\n",
    "validate by concatenating supposedly identical results and then deduplicating that stuff ...\n",
    "\n",
    "good outcome should be EMPTY \n",
    "\n",
    "I.E. ALL ENTRIES HAVE THEIR IDENTICAL \"TWINS\" ...\n",
    "\n",
    "\n",
    "we've got a bunch of DataFrames to compare (legacy stuff):\n",
    " - `peaks_tiles_concat` with locally adjusted expected calculated using legacy `get_adjusted_expected_tile` (diag-chunks)\n",
    " - `peaks_nans` calculated using legacy `get_adjusted_expected_some_nans` (entire chrom as a single piece)\n",
    " - `peaks_original` caluclated using legacy `get_adjusted_expected` (entire chrom as a single piece, the most ORIGINAL one)\n",
    " \n",
    "new DataFrames :\n",
    " - `new_peaks_entire_chrom` calculated using the newest `get_adjusted_expected_tile_some_nans` for entire chrom in 1 piece.\n",
    " - `new_peaks_diag_tiles` calculated using the newest `get_adjusted_expected_tile_some_nans` using diagonal chunking/tiling.\n",
    " - `new_peaks_square_tiles` calculated using the newest `get_adjusted_expected_tile_some_nans` using square chunking/tiling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [row, col]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = peaks_tiles_concat[['row','col']]\n",
    "b = peaks_nans[['row','col']]\n",
    "c = peaks_original[['row','col']]\n",
    "\n",
    "d = new_peaks_entire_chrom[['row','col']]\n",
    "e = new_peaks_diag_tiles[['row','col']]\n",
    "f = new_peaks_square_tiles[['row','col']]\n",
    "\n",
    "print(pd.concat([a,b,c,d,e,f]).drop_duplicates(keep=False))\n",
    "\n",
    "# ##################\n",
    "# # compatibility restored\n",
    "# # no difference detected\n",
    "# ##################\n",
    "# print(pd.concat([aaa,bbb,ccc,ddd,eee,fff]).drop_duplicates(keep=False))\n",
    "# # print(pd.concat([xxx,www]).drop_duplicates(keep=False))\n",
    "\n",
    "\n",
    "# #####################\n",
    "# # RESULTS IS SIPPOSED TO BE EMPTY\n",
    "# # IF EVERYHTING IS OK !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
